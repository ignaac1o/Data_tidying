---
title: "Task on Modeling"
author: "Ignacio AlmodÃ³var & Alejandra Estrada"
date: "03/22/2022"
output: pdf_document
---

```{r setup, include=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(magrittr)
library(ggplot2)
library(h2o)
library(tidymodels)
library(infer)
set.seed(123)
```

\newpage

## Data Explanation

We have chosen a dataset located in kaggle. It contains information about the classification of certain drug types based on different features such as the age, the sex, the blood pressure levels, the cholesterol levels and the sodium-to-potassium ratio.

```{r}
data=read.csv2("drug200.csv",sep = ",",header = TRUE)
data %>% head(5)
```

As it can be seen, most of the variables are categorical with a few levels. Indeed "Age" and "Na_to_K" are the only continuous ones. The last column "Drug" is the response variable, therefore, we will base most of out analysis in this variable.


## Preprocess 

First of all we have to see if the dataset that we are using has any missing values. Therefore, we have created a function that tells us if there is any missing values.


```{r,echo=FALSE}
missingValues=function(data){ 
  count=0
  a=cbind(lapply(lapply(data, is.na), sum)) 
  for(i in 1:ncol(data)){
    if(a[i]!=0){
      cat("There are", a[i], "missing values in column ", i,"\n" )
      count=count+1
    } 
  }
  if(count==0){
    cat("There are no missing values in this dataset")
  } 
}
```

```{r}
missingValues(data)
```

Then it is always interesting to see which types of object is assigned to each variable. To do so we will quickly summarize the dataset.

```{r}
summary(data)
```

As we can see, the summary says that all variables except Age are characters. However, analyzing the data the variable "Na_to_k" looks like a numeric variable so we must change it. Also, the variable "Drug" which is the one that we are going to base our analysis on is defined as a character but we want it to be a factor.

```{r}
data[,5]  %<>% as.numeric()
data[,6] %<>% as.factor()
summary(data)
```

Now we know that the data has the correct format. Therefore, we can start analyzing it. First of all we are going to plot the continuous variables to see if we can find any group evidences for the type of drug given the age and the concentration Na_to_K

```{r,out.width="70%"}
ggplot(data,aes(Age,Na_to_K,col=Drug)) + geom_point()
```

Beforehand there is not clear evidence for the differentiation in groups given the age and the Na_t_k. However, as can be seen, for the DrugY there is a clear bandwidth for Na_to_k being higher than 15.

## H2O

We want to be able to predict the type of Drug that each person should be taken based on our predictive variables "Age","Sex","BP","Cholesterol" and "Na_to_K". Therefore, we are clearly facing a multiclass classification problem, so we would need to find and fit a model that suits our data and is able to predict and classify with a good accuracy.

With this said we are first going to fit a model using h2o package. This package is very useful as it computes many models for the data given and gives many outputs indicating several aspects of the model.

```{r,results=FALSE,include=FALSE}
table(data$Drug)
h2o.init()
data_h2o=as.h2o(data)
resp_data="Drug"
pred_data=setdiff(names(data_h2o), resp_data)

setdiff(names(data_h2o), resp_data)
data_h2o[, resp_data] <- as.factor(data_h2o[, resp_data])

splits = h2o.splitFrame(data = data_h2o, ratios = 0.8, seed = 42)
train = splits[[1]]
test = splits[[2]]

# Run AutoML
aml_mul = h2o.automl(x = pred_data, y = resp_data, training_frame = train, leaderboard_frame = test,
 include_algos = c("GLM", "XGBoost", "DeepLearning","DRF","GBM","StackedEnsemble"),max_runtime_secs = 100,
 max_runtime_secs_per_model = 100, verbosity = NULL)

```

So, we build an h2o and split the data into training and testing. Then run the h2o and obtain the following leaderboard of the methods computed.

```{r}
h2o.get_leaderboard(aml_mul) %>% head(5)
```

As we can see, the classification is not good at all. It is important to meassure that we restricted a lot the time taken per model because the dataset is not long so at the end the computations for each model should not take too long. Nevertheless we can see that the model with the lowest "mean_per_class_error" is the DeepLearning one. Therefore we can say that this is the best model calculated with the h2o package. Thus it will be the one used for predicting.

Even though we know that the model is not very good we want to check its prediction. Therefore, we compute it using the function given in h2o predict() and build a confusion matrix to see the accuracy with the real Drug type of the test data.

```{r}
h2o.no_progress()
pred_mul <- h2o.predict(object = aml_mul, newdata = test)
labels_mul <- as.matrix(pred_mul$predict)
table(labels_mul, as.matrix(test$Drug))
```

As we expected, the classification has not been good at all, indeed the only label that has been assigned more times correctly is that of the variable "DrugY", which is the one most repeated in the dataset and the one that could be easily classified by just looking at the first plot that we did. The rest labels give us very unfavorable results. Some of them are not being predicted correctly neither for just one time.

Therefore, we can conclude that none of these models is very useful since the classification is poor. To end, we will see which of the variables were more important for the model.

```{r}
ex_mul <- h2o.explain(object = aml_mul, newdata = test)
```

The explainers clearly point as expected to "Na_to_K" being the most relevant predictor. The partial dependence plots are not so useful in this case. To finally end with the h2o analysis we have to close the cluster:

```{r}
h2o.shutdown(prompt = FALSE)
```

## Tidymodels

We are now going to continue with our analysis but now using some libraries and functions from the package "tidymodels". This is a very versatile package, it includes functions for basically facing every kind of statistical problems.

We can know have a quick-view of our data with the function glimpse(). It is the same data that we have been using in the section before, therefore we already know enough about it. However, before beginning an analysis it is always good to make sure that the data has the correct format.


```{r}
glimpse(data)
```


Probably one of the most useful packages that tidy includes is "rsample" within it we can draw different samples from the main data and work with them to do different analysis. To test an work with this library we are going to use bootstraps() to obtain observations of the dataset with replacement. With these datasets we are going to fit different multinomial models for multiclass classification also using a tidy package parnsip, which helps to build several models.

Therefore we first create the bootstrap object and ask for 5 different samples. We can access to each sample with analysis() function. Also, with the assesment() function we can access to what will be our "test data".

```{r}
boot_data <- bootstraps(data, times = 5,apparent = TRUE)
analysis(boot_data$splits[[1]]) %>% head(4)
```

We now want to build a model that allows to do classification over our dataset. To do that we are going to use a multinomial regression model with a neural network engine. 

```{r}
?details_multinom_reg_nnet
mult_reg=multinom_reg() %>% set_engine("nnet") %>% set_mode("classification")
```

Once we have the model set up we can fit it with the different samples.

```{r}
boot_fit=list()
for (i in 1:5) {
  boot_fit[[i]] <- mult_reg %>%
    fit(Drug ~ ., data = analysis(boot_data$splits[[i]]))
}
```

Then we can evaluate them with the test data also given by the bootstrap function. We are going to do different predictions for each model and then compare it with its test to see how well the classification was done.

```{r}
prediction=list()
prediction_percentage=c()
for (i in 1:5) {
  prediction[i]=predict(boot_fit[[i]],assessment(boot_data$splits[[i]]))
}

for (i in 1:5) {
  prediction_percentage[i]=mean(prediction[[i]] == assessment(boot_data$splits[[i]])$Drug)
}

prediction_percentage
```

As we can see from the percentages for predictions that were right, most of the models are actually very good. In fact, all of them are over 90%, which means that it is predicting almost perfectly. 

However, this bootstrap example was done just for educational purposes. It is usually better to use most of the data without replacement available to fit a model. Also, as we saw that using this approach we could get over an 98% of the prediction correctly, we would now try another method but now using all the data available.

We are now going to split again the dataset. But this time into training and testing, again using the rsample package.

```{r}
drugs_split=initial_split(data,strata = Na_to_K,prop = 0.7)
drugs_train <- training(drugs_split)
drugs_test <- testing(drugs_split)
```

And now we create another model. We are going to go now with a boost_tree model, using the engine "xgboost" for multiclass classification. Again, we create and fit the model:

```{r,warning=FALSE,results=FALSE}
tree_spec=boost_tree() %>% set_engine("xgboost") %>% set_mode("classification")

tree_fit <- tree_spec %>%
  fit(Drug ~ ., data = drugs_train)
```

Once we have the model fitted we can check its accuracy for predictions. To do that we use predict and obtain its confusion matrix.

```{r}
test_results <- bind_cols(
  dplyr::select(drugs_test, "Drug"),
  predict(tree_fit, drugs_test))

table(test_results$Drug, test_results$.pred_class)
```

From the output we can see that the prediction has an accuracy of almost 100%, which means that our model is working very good. Indeed it is able to predict perfectly the type of Drug that a person should be taking given the predictors seen in this analysis.

Once we have a good predictive model we are going to do a little bit of inference on our dataset. To do this we are going to use the "infer" package, which is also part of tidymodels. We are going to analyze if the mean age of the sample is anywhere close to the real population, where we know that the mean age in 2015 was 42.5 in Spain which will be our null hypothesis.

```{r}
age_estimator = data %>% specify(response=Age) %>% calculate(stat = "mean")
null_dist <- data %>% specify(response=Age) %>%
   hypothesize(null = "point",mu=42.5) %>%
   generate(reps = 1000, type = "bootstrap") %>%
   calculate(stat = "mean")

visualize(null_dist) +
  shade_p_value(obs_stat = age_estimator, direction = "two_sided")
```

From the plot we can see that the mean population of the sample lies on a tail of the null hypothesis. Therefore, observing the sample mean of 44.315 looks a little bit unlikely. However, we still have to check how unlikely this is. To obtain so we calculate the p-value.

```{r}
p_value <- null_dist %>%
  get_p_value(obs_stat = age_estimator, direction = "two_sided")
p_value
```

We can see that the p_value obtained is small, but not very much. Which means, that the probability of the sample mean being this far from 42.5 is 0.13. Nevertheless, we still have to calculate the confidence interval for these results. We will calculate it for a confidence level of $\alpha=0.05$.

```{r}
null_dist %>% get_confidence_interval(point_estimate = age_estimator, level = .95, type = "se")
```

As it can be seen, 42.5 lies inside this interval, which means that the p-value is not statistically significant at an $\alpha=0.05$ confidence level.

