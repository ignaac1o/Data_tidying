---
title: "Task on Modeling"
author: "Ignacio Almodóvar & Alejandra Estrada"
date: "3/22/2022"
output: pdf_document
---

```{r setup, include=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(magrittr)
library(ggplot2)
library(h2o)
library(tidymodels)
library(unbalanced)
```

```{r}
data=read.csv2("drug200.csv",sep = ",",header = TRUE)
summary(data)
data %>% head(5)
```

## Preprocess 

let's search for missing values.

```{r}
missingValues=function(data){ 
  count=0
  a=cbind(lapply(lapply(data, is.na), sum)) 
  for(i in 1:ncol(data)){
    if(a[i]!=0){
      cat("There are", a[i], "missing values in column ", i,"\n" )
      count=count+1
    } 
  }
  if(count==0){
    cat("There are no missing values in this dataset")
  } 
}
missingValues(drugs)
```


As we can see, the summary says that all variables except Age are factors. However, analyzing the data the variable "Na_to_k" looks like a numeric variable so we must change it.

```{r}
data[,5]  %<>% as.numeric()
summary(data)
```


#Analyze correlation (hacer para variable age y Na_to_K ya que son las únicas continuas)
No se si se podrá distinguir entre tipos de droga para las correlaciones o que. Pero estaría bien ver si se puede sacar alguna conclusion interseante con la correlación.



We now plot the continuous variables to see if we can find any group evidences for the type of drug


```{r}
ggplot(data,aes(Age,Na_to_K,col=Drug)) + geom_point()
```

Beforehand there is not clear evidence for the differentiation in groups given the age and the Na_t_k. However, as can be seen, for the DrugY there is a clear bandwidth for Na_to_k being higher than 15.

We are now going to fitt a classification model using h2o package.

```{r,results=FALSE}
table(data$Drug)
h2o.init()
data_h2o=as.h2o(data)
resp_data="Drug"
pred_data=setdiff(names(data_h2o), resp_data)


setdiff(names(data_h2o), resp_data)
data_h2o[, resp_data] <- as.factor(data_h2o[, resp_data])

splits = h2o.splitFrame(data = data_h2o, ratios = 0.8, seed = 42)
train = splits[[1]]
test = splits[[2]]
# Run AutoML
aml_mul = h2o.automl(x = pred_data, y = resp_data, training_frame = train, leaderboard_frame = test,
                      include_algos = c("GLM", "XGBoost", "DeepLearning","DRF","GBM","StackedEnsemble"),max_runtime_secs = 1000,max_runtime_secs_per_model = 100, verbosity = NULL)


```


```{r}
lb_mul <- h2o.get_leaderboard(aml_mul)
print(lb_mul, n = nrow(lb_mul))
```
```{r}
probs <- table(as.matrix(data$Drug))
probs <- probs / sum(probs)
(sum(probs^2))

(mean(1 - probs))
```


```{r}
pred_mul <- h2o.predict(object = aml_mul, newdata = test)
h2o.head(pred_mul)

labels_mul <- as.matrix(pred_mul$predict)
table(labels_mul, as.matrix(test$Drug))

h2o.shutdown(prompt = FALSE)
```
 Como podemos ver ninguno de estos modelos es muy util ya que la clasificación es mala. 


## Tidymodels

```{r}
#Instead of summary
glimpse(data)
data %>% skim()
```



```{r}
boot_data <- bootstraps(data, times = 10)
analysis(boot_data$splits[[1]] )%>% head()
```

#rsample

To split data into training and testing we are going to use the rsample package.

```{r}
drugs_split=initial_split(data,strata = Na_to_K,prop = 0.7)
table(training(drugs_split)$Drug)

```


```{r}
data$Drug %<>% as.factor() 
data$Na_to_K %<>% as.numeric()
summary(data)
boot_data=bootstraps(data,times=10)
boot_fit=list()
for (i in 1:10) {
  boot_fit[[i]] <- mnr_spec %>%
    fit(Drug ~ ., data = analysis(boot_data$splits[[i]]))
}



```


### Parsnip

```{r}
library(tidymodels)

# Create an initial split stratifying by the response
set.seed(42)
data_split <- initial_split(data, prop = 0.75)
ames_train <- training(data_split)
ames_test <- testing(data_split)

ames_train$Drug %<>% as.factor()
```


```{r}
mnr_spec <- multinom_reg(penalty = 0.1) %>%
  set_engine("nnet")
mnr_spec


mnr_fit <- mnr_spec %>%
  fit(Drug ~ ., data = training(drugs_split))
mnr_fit


test_results <- bind_cols(
  dplyr::select(ames_test, "Drug"),
  predict(mnr_fit, ames_test),
  predict(mnr_fit, ames_test, type = "prob")
)

table(test_results$Drug, test_results$.pred_class)

mean(test_results$Drug == test_results$.pred_class, na.rm = TRUE)
```




```{r}
tree_spec=boost_tree() %>% set_engine("xgboost") %>% set_mode("classification")

tree_fit <- tree_spec %>%
  fit(Drug ~ ., data = ames_train)
tree_fit

test_results <- bind_cols(
  dplyr::select(ames_test, "Drug"),
  predict(tree_fit, ames_test),
  predict(tree_fit, ames_test, type = "prob")
)

table(test_results$Drug, test_results$.pred_class)

mean(test_results$Drug == test_results$.pred_class, na.rm = TRUE)

```




### Discrim

```{r}
library(discrim)
# Fit a Naive Bayes model (which is actually a kernel discriminant # analysis done by combining univariate kernel density estimators) 

summary(data)
data$Drug %<>% as.factor() 
nb_mod <- naive_Bayes() %>%
  set_engine("naivebayes") %>%
  fit(Drug ~ ., data = ames_train)
nb_mod


data_grid <- expand.grid(Age = seq(1, 100, length = 200),
                         Na_to_K = seq(6, 38, length = 200))

data_grid$classes_nb <- predict(nb_mod, ames_test)$.pred_class


ggplot(data, aes(x = Na_to_K, y = Age)) +
  geom_point(aes(col = Drug, pch = Drug), alpha = 0.75) +
  geom_raster(data = data_grid,
              aes(x = Na_to_K, y = Age, fill = classes_nb),
              alpha = 0.25) +
  theme_bw() +
  theme(legend.position = "top") +
  coord_equal()

```

