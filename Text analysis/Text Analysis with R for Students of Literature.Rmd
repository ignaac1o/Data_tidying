---
title: "Text Analysis with R for Students of Literature"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 0. Preprocessing 

### Loading the first text file

```{r,comment=NA}
library(quanteda)
library(readtext)
data_macbeth <- texts(readtext("https://www.gutenberg.org/files/1533/1533-0.txt"))
names(data_macbeth) <- "Macbeth"

library(stringi)
stri_sub(data_macbeth, 1, 65)
```

### Separate content from metadata

Extract the header information

```{r,comment=NA}
(start_v <- stri_locate_first_fixed(data_macbeth, "SCENE I. An open Place.")[1])
(end_v <- stri_locate_last_fixed(data_macbeth, "[_Flourish. Exeunt._]")[1])
```

Verify that "[_Flourish. Exeunt._]" is the end of the novel

```{r,comment=NA}
kwic(tokens(data_macbeth), "[_Flourish. Exeunt._]")

stri_count_fixed(data_macbeth, "\n")

stri_sub(data_macbeth, from = start_v, to = end_v) %>%
  stri_count_fixed("\n")

novel_v <- stri_sub(data_macbeth, start_v, end_v)
novel_v = gsub("€", "", novel_v)
novel_v = gsub("™", "", novel_v)
length(novel_v)

stri_sub(novel_v, 1, 70) %>% cat()
```

### Reprocessing the content

Lowercase text

```{r,comment=NA}
novel_lower_v <- char_tolower(novel_v)

macbeth_word_v <- tokens(novel_lower_v, remove_punct = TRUE) %>% as.character()
(total_length <- length(macbeth_word_v))

macbeth_word_v[1:11]

macbeth_word_v[9999] 

macbeth_word_v[c(6,7,8)]
```

Check positions of "love"

```{r,comment=NA}
which(macbeth_word_v == "love") %>% head()
```

## 1. Analyse and study the occurrence of words related with love or positive feelings in general.

### Beginning the analysis

```{r,comment=NA}
length(macbeth_word_v[which(macbeth_word_v == "love")])
```

Same thing using kwic()

```{r,comment=NA}
nrow(kwic(novel_lower_v, pattern = "love"))

nrow(kwic(novel_lower_v, pattern = "love*")) # Includes words like "whalemen"

(total_love_hits <- nrow(kwic(novel_lower_v, pattern = "^love{0,1}$", valuetype = "regex")))

total_love_hits / ntoken(novel_lower_v, remove_punct = TRUE)
```

Total unique words

```{r,comment=NA}
length(unique(macbeth_word_v))

ntype(char_tolower(novel_v), remove_punct = TRUE)
```

## 2. Make frequency plots.

Ten most frequent words

```{r,comment=NA}
macbeth_dfm <- dfm(novel_lower_v, remove_punct = TRUE)

head(macbeth_dfm, nf = 10)

library("quanteda.textstats")
textstat_frequency(macbeth_dfm, n = 10)
```

Plot frequency of 50 most frequent terms

```{r,comment=NA}
library(ggplot2)
theme_set(theme_minimal())
textstat_frequency(macbeth_dfm, n = 50) %>% 
  ggplot(aes(x = rank, y = frequency)) +
  geom_point() +
  labs(x = "Frequency rank", y = "Term frequency")

sorted_macbeth_freqs_t <- topfeatures(macbeth_dfm, n = nfeat(macbeth_dfm))
```

## 3. Compare word frequency data of words like “he”, “she”, “him”, “her” and show also relative frequencies.

### Accessing Word Data

Frequencies of "he" and "she" - these are matrixes, not numerics

```{r,comment=NA}
sorted_macbeth_freqs_t[c("he", "she", "him", "her")]
```

Another method: indexing the dfm

```{r,comment=NA}
macbeth_dfm[, c("he", "she", "him", "her")]

sorted_macbeth_freqs_t[1]

sorted_macbeth_freqs_t["the"]
```

Term frequency ratios

```{r,comment=NA}
sorted_macbeth_freqs_t["him"] / sorted_macbeth_freqs_t["her"]

sorted_macbeth_freqs_t["he"] / sorted_macbeth_freqs_t["she"]

ntoken(macbeth_dfm)

sum(sorted_macbeth_freqs_t)
```

## 2. Make frequency plots

### Recycling

```{r,comment=NA}
sorted_macbeth_rel_freqs_t <- sorted_macbeth_freqs_t / sum(sorted_macbeth_freqs_t) * 100
sorted_macbeth_rel_freqs_t["the"]
```

By weighting the dfm directly

```{r,comment=NA}
macbeth_dfm_pct <- dfm_weight(macbeth_dfm, scheme = "prop") * 100

dfm_select(macbeth_dfm_pct, pattern = "the")

plot(sorted_macbeth_rel_freqs_t[1:10], type = "b",
     xlab = "Top Ten Words", ylab = "Percentage of Full Text", xaxt = "n")
axis(1,1:10, labels = names(sorted_macbeth_rel_freqs_t[1:10]))

textstat_frequency(macbeth_dfm_pct, n = 10) %>% 
  ggplot(aes(x = reorder(feature, -rank), y = frequency)) +
  geom_bar(stat = "identity") + coord_flip() + 
  labs(x = "", y = "Term Frequency as a Percentage")
```

## 4. Make a token distribution analysis.

### Dispersion plots

Using words from tokenized corpus for dispersion

```{r,comment=NA}
library("quanteda.textplots")
textplot_xray(kwic(novel_v, pattern = "macbeth")) + 
  ggtitle("Lexical dispersion")

textplot_xray(
  kwic(novel_v, pattern = "macbeth"),
  kwic(novel_v, pattern = "macduff")) + 
  ggtitle("Lexical dispersion")
```

## 5. Identify chapter breaks.

### Searching with regular expression

Identify the chapter break locations

```{r,comment=NA}
chap_positions_v <- kwic(novel_v, phrase(c("SCENE")), valuetype = "regex")$from

head(chap_positions_v)
chap_positions_v
```

### Identifying chapter breaks

```{r,comment=NA}
chapters_corp <- 
  corpus(novel_v) %>%
  corpus_segment(pattern = "SCENE\\s*.*\\n", valuetype = "regex")
summary(chapters_corp, 10)

docvars(chapters_corp, "pattern") <- stringi::stri_trim_right(docvars(chapters_corp, "pattern"))
summary(chapters_corp, n = 3)

docnames(chapters_corp) <- docvars(chapters_corp, "pattern")
```

### Barplots of Macbeth and Macduff

Create a dfm

```{r,comment=NA}
chap_dfm <- dfm(chapters_corp)
```

Extract row with count for "whale"/"ahab" in each chapter and convert to data frame for plotting

```{r,comment=NA}
macbeth_macduff_df <- chap_dfm %>% 
  dfm_keep(pattern = c("macbeth", "macduff")) %>% 
  convert(to = "data.frame")

macbeth_macduff_df$chapter <- 1:nrow(macbeth_macduff_df)

ggplot(data = macbeth_macduff_df, aes(x = chapter, y = macbeth)) + 
  geom_bar(stat = "identity") +
  labs(x = "Chapter", 
       y = "Frequency",
       title = 'Occurrence of "Macbeth"')

ggplot(data = macbeth_macduff_df, aes(x = chapter, y = macduff)) + 
  geom_bar(stat = "identity") +
  labs(x = "Chapter", 
       y = "Frequency",
       title = 'Occurrence of "Macduff"')

rel_dfm <- dfm_weight(chap_dfm, scheme = "prop") * 100
head(rel_dfm)
```

Subset dfm and convert to data.frame object

```{r,comment=NA}
rel_chap_freq <- rel_dfm %>% 
  dfm_keep(pattern = c("macbeth", "macduff")) %>% 
  convert(to = "data.frame")

rel_chap_freq$chapter <- 1:nrow(rel_chap_freq)
ggplot(data = rel_chap_freq, aes(x = chapter, y = macbeth)) + 
  geom_bar(stat = "identity") +
  labs(x = "Chapter", y = "Relative frequency",
       title = 'Occurrence of "Macbeth"')

ggplot(data = rel_chap_freq, aes(x = chapter, y = macduff)) + 
  geom_bar(stat = "identity") +
  labs(x = "Chapter", y = "Relative frequency",
       title = 'Occurrence of "Macduff"')
```

## 6. Only if you have some knowledge about the novel: Make a correlation analysis between words related with love or positive feelings and some particular characters or people of the novel.

### Correlation Analysis

```{r,comment=NA}
dfm_weight(chap_dfm, scheme = "prop") %>% 
  textstat_simil(selection = c("macbeth", "macduff"), method = "correlation", margin = "features") %>%
  as.matrix() %>%
  head(2)
```

### Testing Correlation with Randomization+

```{r,comment=NA}
cor_data_df <- dfm_weight(chap_dfm, scheme = "prop") %>% 
  dfm_keep(pattern = c("macbeth", "macduff")) %>% 
  convert(to = "data.frame")
```

Sample 1000 replicates and create data frame

```{r,comment=NA}
n <- 1000
samples <- data.frame(
  cor_sample = replicate(n, cor(sample(cor_data_df$macbeth), cor_data_df$macduff)),
  id_sample = 1:n
)
```

Plot distribution of resampled correlations

```{r,comment=NA}
ggplot(data = samples, aes(x = cor_sample, y = ..density..)) +
  geom_histogram(colour = "black", binwidth = 0.01) +
  geom_density(colour = "red") +
  labs(x = "Correlation Coefficient", y = NULL,
       title = "Histogram of Random Correlation Coefficients with Normal Curve")
```


## 7. Show some measures of lexical variety.

### Mean word frequency

Length of the book in chapters

```{r,comment=NA}
ndoc(chapters_corp)
```

Chapter names

```{r,comment=NA}
docnames(chapters_corp) %>% head()
```

For first few chapters
```{r,comment=NA}
ntoken(chapters_corp) %>% head()
```

Average
```{r,comment=NA}
(ntoken(chapters_corp) / ntype(chapters_corp)) %>% head()
```

### Extracting Word Usage Means

```{r,comment=NA}
(ntoken(chapters_corp) / ntype(chapters_corp)) %>%
  plot(type = "h", ylab = "Mean word frequency")

(ntoken(chapters_corp) / ntype(chapters_corp)) %>%
  scale() %>%
  plot(type = "h", ylab = "Scaled mean word frequency")
```

### Ranking the values

```{r,comment=NA}
mean_word_use_m <- (ntoken(chapters_corp) / ntype(chapters_corp))
sort(mean_word_use_m, decreasing = TRUE) %>% head()
```

### Calculating the TTR

```{r,comment=NA}
dfm(chapters_corp) %>% 
  textstat_lexdiv(measure = "TTR") %>%
  head(n = 10)
```

## 8. Calculate the Hapax Richness.

Hapaxes per document
```{r,comment=NA}
rowSums(chap_dfm == 1) %>% head()
```

As a proportion
```{r,comment=NA}
hapax_proportion <- rowSums(chap_dfm == 1) / ntoken(chap_dfm)
head(hapax_proportion)

barplot(hapax_proportion, beside = TRUE, col = "grey", names.arg = seq_len(ndoc(chap_dfm)))
```