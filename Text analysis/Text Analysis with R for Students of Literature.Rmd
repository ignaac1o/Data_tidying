---
title: "Text Analysis with R for Students of Literature"
author: "Ignacio Almodovar Cárdenas and Alejandra Estrada Sanz"
date: "22/03/2022"
output: 
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

## 0. Preprocessing.

Loading the first text file.

```{r,comment=NA,warning=FALSE}
library(quanteda)
library(readtext)
library(stringi)
data_macbeth <- texts(readtext("https://www.gutenberg.org/files/1533/1533-0.txt"))
names(data_macbeth) <- "Macbeth"
```

Separate content from metadata and extract the header information.

```{r,comment=NA,warning=FALSE}
start_v <- stri_locate_first_fixed(data_macbeth, "SCENE I. An open Place.")[1]
end_v <- stri_locate_last_fixed(data_macbeth, "[_Flourish. Exeunt._]")[1]
novel_v <- stri_sub(data_macbeth, start_v, end_v)
novel_v = gsub("€", "", novel_v)
novel_v = gsub("™", "", novel_v)
```

Reprocessing the content for lowercase text.

```{r,comment=NA,warning=FALSE}
novel_lower_v <- char_tolower(novel_v)
macbeth_word_v <- tokens(novel_lower_v, remove_punct = TRUE) %>% as.character()
total_length <- length(macbeth_word_v)
```

## 1. Analyse and study the occurrence of words related with love or positive feelings in general.

Beginning the analysis.

```{r,comment=NA,warning=FALSE}
length(macbeth_word_v[which(macbeth_word_v == "love")])
```

Same thing using kwic().

```{r,comment=NA,warning=FALSE}
nrow(kwic(novel_lower_v, pattern = "love"))
nrow(kwic(novel_lower_v, pattern = "love*")) # Includes words like "love"
(total_love_hits <- nrow(kwic(novel_lower_v, pattern = "^love{0,1}$", valuetype = "regex")))
total_love_hits / ntoken(novel_lower_v, remove_punct = TRUE)
```

## 2. Make frequency plots.

Ten most frequent words.

```{r,comment=NA,warning=FALSE}
macbeth_dfm <- dfm(novel_lower_v, remove_punct = TRUE)
library("quanteda.textstats")
textstat_frequency(macbeth_dfm, n = 10)
```

Plot frequency of 50 most frequent terms.

```{r,comment=NA,warning=FALSE}
library(ggplot2)
theme_set(theme_minimal())

textstat_frequency(macbeth_dfm, n = 50) %>% 
  ggplot(aes(x = rank, y = frequency)) +
  geom_point() +
  labs(x = "Frequency rank", y = "Term frequency")

sorted_macbeth_freqs_t <- topfeatures(macbeth_dfm, n = nfeat(macbeth_dfm))
```

Recycling.

```{r,comment=NA,warning=FALSE}
sorted_macbeth_rel_freqs_t <- sorted_macbeth_freqs_t / sum(sorted_macbeth_freqs_t) * 100
```

By weighting the dfm directly.

```{r,comment=NA,warning=FALSE}
macbeth_dfm_pct <- dfm_weight(macbeth_dfm, scheme = "prop") * 100

plot(sorted_macbeth_rel_freqs_t[1:10], type = "b",
     xlab = "Top Ten Words", ylab = "Percentage of Full Text", xaxt = "n")
axis(1,1:10, labels = names(sorted_macbeth_rel_freqs_t[1:10]))

textstat_frequency(macbeth_dfm_pct, n = 10) %>% 
  ggplot(aes(x = reorder(feature, -rank), y = frequency)) +
  geom_bar(stat = "identity") + coord_flip() + 
  labs(x = "", y = "Term Frequency as a Percentage")
```

## 3. Compare word frequency data of words like “he”, “she”, “him”, “her” and show also relative frequencies.

Frequencies of "he", "she", "him" and "her".

```{r,comment=NA,warning=FALSE}
sorted_macbeth_freqs_t[c("he", "she", "him", "her")]
```

Another method: indexing the dfm.

```{r,comment=NA,warning=FALSE}
macbeth_dfm[, c("he", "she", "him", "her")]
```

Term frequency ratios.

```{r,comment=NA,warning=FALSE}
sorted_macbeth_freqs_t["him"] / sorted_macbeth_freqs_t["her"]
sorted_macbeth_freqs_t["he"] / sorted_macbeth_freqs_t["she"]
```

## 4. Make a token distribution analysis.

Dispersion plots using words from tokenized corpus for dispersion.

```{r,comment=NA,warning=FALSE}
library("quanteda.textplots")
textplot_xray(kwic(novel_v, pattern = "macbeth")) + 
  ggtitle("Lexical dispersion")

textplot_xray(
  kwic(novel_v, pattern = "macbeth"),
  kwic(novel_v, pattern = "macduff")) + 
  ggtitle("Lexical dispersion")
```

## 5. Identify chapter breaks.

Identify the chapter break locations.

```{r,comment=NA,warning=FALSE}
chap_positions_v <- kwic(novel_v, phrase(c("SCENE")), valuetype = "regex")$from
head(chap_positions_v)
chap_positions_v
```

Identifying chapter breaks.

```{r,comment=NA,warning=FALSE}
chapters_corp <- 
  corpus(novel_v) %>%
  corpus_segment(pattern = "SCENE\\s*.*\\n", valuetype = "regex")
summary(chapters_corp, 10)
docvars(chapters_corp, "pattern") <- stringi::stri_trim_right(docvars(chapters_corp, "pattern"))
docnames(chapters_corp) <- docvars(chapters_corp, "pattern")
```

## 7. Show some measures of lexical variety.

Mean word frequency.

Length of the book in chapters.

```{r,comment=NA,warning=FALSE}
ndoc(chapters_corp)
```

Chapter names.

```{r,comment=NA,warning=FALSE}
docnames(chapters_corp) %>% head()
```

For first few chapters.

```{r,comment=NA,warning=FALSE}
ntoken(chapters_corp) %>% head()
```

Average.

```{r,comment=NA,warning=FALSE}
(ntoken(chapters_corp) / ntype(chapters_corp)) %>% head()
```

Extracting Word Usage Means.

```{r,comment=NA,warning=FALSE}
(ntoken(chapters_corp) / ntype(chapters_corp)) %>%
  plot(type = "h", ylab = "Mean word frequency")

(ntoken(chapters_corp) / ntype(chapters_corp)) %>%
  scale() %>%
  plot(type = "h", ylab = "Scaled mean word frequency")
```

Ranking the values.

```{r,comment=NA,warning=FALSE}
mean_word_use_m <- (ntoken(chapters_corp) / ntype(chapters_corp))
sort(mean_word_use_m, decreasing = TRUE) %>% head()
```

Calculating the TTR.

```{r,comment=NA,warning=FALSE}
dfm(chapters_corp) %>% 
  textstat_lexdiv(measure = "TTR") %>%
  head(n = 10)
```

## 8. Calculate the Hapax Richness.

Create a dfm.

```{r,comment=NA,warning=FALSE}
chap_dfm <- dfm(chapters_corp)
```

Hapaxes per document.

```{r,comment=NA,warning=FALSE}
rowSums(chap_dfm == 1) %>% head()
```

As a proportion.

```{r,comment=NA,warning=FALSE}
hapax_proportion <- rowSums(chap_dfm == 1) / ntoken(chap_dfm)
head(hapax_proportion)

barplot(hapax_proportion, beside = TRUE, col = "grey", names.arg = seq_len(ndoc(chap_dfm)))
```