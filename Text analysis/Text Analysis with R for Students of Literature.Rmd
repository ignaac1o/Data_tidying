---
title: "Text Analysis with R for Students of Literature"
author: "Ignacio Almodovar Cárdenas and Alejandra Estrada Sanz"
date: "22/03/2022"
output: 
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

## 0. Preprocessing

First of all, we load our text file, we are going to analyze the book "Macbeth".

```{r,comment=NA,warning=FALSE}
library(quanteda)
library(readtext)
library(stringi)
data_macbeth <- texts(readtext("https://www.gutenberg.org/files/1533/1533-0.txt"))
names(data_macbeth) <- "Macbeth"
```

Then, we separate content from metadata and extract the header and final leftover information.

```{r,comment=NA,warning=FALSE}
start_v <- stri_locate_first_fixed(data_macbeth, "SCENE I. An open Place.")[1]
end_v <- stri_locate_last_fixed(data_macbeth, "[_Flourish. Exeunt._]")[1]
novel_v <- stri_sub(data_macbeth, start_v, end_v)
novel_v = gsub("€", "", novel_v)
novel_v = gsub("™", "", novel_v)
```

Finally, we reprocess the content for lowercase text.

```{r,comment=NA,warning=FALSE}
novel_lower_v <- char_tolower(novel_v)
macbeth_word_v <- tokens(novel_lower_v, remove_punct = TRUE) %>% as.character()
total_length <- length(macbeth_word_v)
```

## 1. Analyse and study the occurrence of words related with love or positive feelings in general

We count the number of times the word "love" is repeated.

```{r,comment=NA,warning=FALSE}
length(macbeth_word_v[which(macbeth_word_v == "love")])
```

We do the same thing using kwic().

```{r,comment=NA,warning=FALSE}
nrow(kwic(novel_lower_v, pattern = "love"))
```

We also count the number of times that words similar to "love" are repeated.

```{r,comment=NA,warning=FALSE}
nrow(kwic(novel_lower_v, pattern = "love*"))
```

And we get the ratio of "love" to the total number of words.

```{r,comment=NA,warning=FALSE}
total_love_hits <- nrow(kwic(novel_lower_v, pattern = "^love{0,1}$", valuetype = "regex"))
total_love_hits / ntoken(novel_lower_v, remove_punct = TRUE)
```

## 2. Make frequency plots

We obtain the ten most frequent words.

```{r,comment=NA,warning=FALSE}
macbeth_dfm <- dfm(novel_lower_v, remove_punct = TRUE)
library("quanteda.textstats")
textstat_frequency(macbeth_dfm, n = 10)
```

And we plot frequency of 50 most frequent terms.

```{r,comment=NA,warning=FALSE}
library(ggplot2)
theme_set(theme_minimal())

textstat_frequency(macbeth_dfm, n = 50) %>% 
  ggplot(aes(x = rank, y = frequency)) +
  geom_point() +
  labs(x = "Frequency rank", y = "Term frequency")

sorted_macbeth_freqs_t <- topfeatures(macbeth_dfm, n = nfeat(macbeth_dfm))
```

We also plot the percentage of full text and the term frequency as a percentage for the ten most frequent words.

```{r,comment=NA,warning=FALSE}
sorted_macbeth_rel_freqs_t <- sorted_macbeth_freqs_t / sum(sorted_macbeth_freqs_t) * 100
macbeth_dfm_pct <- dfm_weight(macbeth_dfm, scheme = "prop") * 100

plot(sorted_macbeth_rel_freqs_t[1:10], type = "b",
     xlab = "Top Ten Words", ylab = "Percentage of Full Text", xaxt = "n")
axis(1,1:10, labels = names(sorted_macbeth_rel_freqs_t[1:10]))

textstat_frequency(macbeth_dfm_pct, n = 10) %>% 
  ggplot(aes(x = reorder(feature, -rank), y = frequency)) +
  geom_bar(stat = "identity") + coord_flip() + 
  labs(x = "", y = "Term Frequency as a Percentage")
```

## 3. Compare word frequency data of words like “he”, “she”, “him”, “her” and show also relative frequencies

We calculate frequencies of "he", "she", "him" and "her".

```{r,comment=NA,warning=FALSE}
sorted_macbeth_freqs_t[c("he", "she", "him", "her")]
```

We do the same with another method, indexing the dfm.

```{r,comment=NA,warning=FALSE}
macbeth_dfm[, c("he", "she", "him", "her")]
```

We also estimate relative frequencies.

```{r,comment=NA,warning=FALSE}
sorted_macbeth_rel_freqs_t["he"]
sorted_macbeth_rel_freqs_t["she"]
sorted_macbeth_rel_freqs_t["him"]
sorted_macbeth_rel_freqs_t["her"]
```

## 4. Make a token distribution analysis

Dispersion plots using words "Macbeth" and "Macduff".

```{r,comment=NA,warning=FALSE}
library("quanteda.textplots")
textplot_xray(kwic(novel_v, pattern = "macbeth")) + 
  ggtitle("Lexical dispersion")

textplot_xray(
  kwic(novel_v, pattern = "macbeth"),
  kwic(novel_v, pattern = "macduff")) + 
  ggtitle("Lexical dispersion")
```

## 5. Identify chapter breaks

We identify the chapter break locations.

```{r,comment=NA,warning=FALSE}
chap_positions_v <- kwic(novel_v, phrase(c("SCENE")), valuetype = "regex")$from
chap_positions_v
```

And we save our chapter breaks in the variable "chapters_corp".

```{r,comment=NA,warning=FALSE}
chapters_corp <- 
  corpus(novel_v) %>%
  corpus_segment(pattern = "SCENE\\s*.*\\n", valuetype = "regex")
summary(chapters_corp, 10)
docvars(chapters_corp, "pattern") <- stringi::stri_trim_right(docvars(chapters_corp, "pattern"))
docnames(chapters_corp) <- docvars(chapters_corp, "pattern")
```

## 7. Show some measures of lexical variety

We want to calculate the mean word frequency, so first, we obtain the length of the book in chapters.

```{r,comment=NA,warning=FALSE}
ndoc(chapters_corp)
```

Then, we obtain chapter names.

```{r,comment=NA,warning=FALSE}
docnames(chapters_corp) %>% head()
```

And calculate mean word frequency for the first few chapters.

```{r,comment=NA,warning=FALSE}
ntoken(chapters_corp) %>% head()
```

We do the same with the average.

```{r,comment=NA,warning=FALSE}
(ntoken(chapters_corp) / ntype(chapters_corp)) %>% head()
```

Then, we extract word usage means.

```{r,comment=NA,warning=FALSE}
(ntoken(chapters_corp) / ntype(chapters_corp)) %>%
  plot(type = "h", ylab = "Mean word frequency")

(ntoken(chapters_corp) / ntype(chapters_corp)) %>%
  scale() %>%
  plot(type = "h", ylab = "Scaled mean word frequency")
```

We also rank the values.

```{r,comment=NA,warning=FALSE}
mean_word_use_m <- (ntoken(chapters_corp) / ntype(chapters_corp))
sort(mean_word_use_m, decreasing = TRUE) %>% head()
```

And finally, we calculate the TTR.

```{r,comment=NA,warning=FALSE}
dfm(chapters_corp) %>% 
  textstat_lexdiv(measure = "TTR") %>%
  head(n = 10)
```

## 8. Calculate the Hapax Richness

First, we create a dfm.

```{r,comment=NA,warning=FALSE}
chap_dfm <- dfm(chapters_corp)
```

We calculate hapaxes per document.

```{r,comment=NA,warning=FALSE}
rowSums(chap_dfm == 1) %>% head()
```

And we calculate them again as a proportion.

```{r,comment=NA,warning=FALSE}
hapax_proportion <- rowSums(chap_dfm == 1) / ntoken(chap_dfm)
head(hapax_proportion)

barplot(hapax_proportion, beside = TRUE, col = "grey", names.arg = seq_len(ndoc(chap_dfm)))
```