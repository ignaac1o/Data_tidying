---
title: "Text Analysis with R for Students of Literature"
author: "Ignacio Almodovar Cárdenas and Alejandra Estrada Sanz"
date: "22/03/2022"
output: 
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(quanteda)
library(readtext)
library(stringi)
library(quanteda.textstats)
library(ggplot2)
library(quanteda.textplots)
```

\newpage

## 0. Preprocessing

First of all, we have to upload our text file. In order to do that we searched for a few books in gutenberg.org and decided to analyze the book "Macbeth".

```{r,comment=NA,warning=FALSE}
data_macbeth <- texts(readtext("https://www.gutenberg.org/files/1533/1533-0.txt"))
names(data_macbeth) <- "Macbeth"
```

Once we have our book ready to analyze into an R object, we can start working on it. This book contains some metadata both at end and start of the book. Therefore, we separate the content from metadata and extract the header and final leftover information.

```{r,comment=NA,warning=FALSE}
start_v <- stri_locate_first_fixed(data_macbeth, "SCENE I. An open Place.")[1]
end_v <- stri_locate_last_fixed(data_macbeth, "[_Flourish. Exeunt._]")[1]
novel_v <- stri_sub(data_macbeth, start_v, end_v)
novel_v = gsub("€", "", novel_v)
novel_v = gsub("™", "", novel_v)
```

Now that we have the text that we have to analyze selected, we can reprocess the content and put everything in lower case.

```{r,comment=NA,warning=FALSE}
novel_lower_v <- char_tolower(novel_v)
macbeth_word_v <- tokens(novel_lower_v, remove_punct = TRUE) %>% as.character()
total_length <- length(macbeth_word_v)
```

## 1. Analyse and study the occurrence of words related with love or positive feelings in general

First let's start by counting the number of times the word "love" is repeated.

```{r,comment=NA,warning=FALSE}
length(macbeth_word_v[which(macbeth_word_v == "love")])
```

We can also do the same thing using kwic() function. This returns a list, therefore we have to count the number of rows to obtain the number of time the word selected is repeated.

```{r,comment=NA,warning=FALSE}
nrow(kwic(novel_lower_v, pattern = "love"))
```

We can also count the number of times that words that begin with "love" are repeated, e.g. "lovers", "loved" or "lover". We can see that there is not much more from words form the family "love".

```{r,comment=NA,warning=FALSE}
nrow(kwic(novel_lower_v, pattern = "love*"))
```

Finally we can also compute the ratio for the word "love" between all the total number of words.

```{r,comment=NA,warning=FALSE}
total_love_hits <- nrow(kwic(novel_lower_v, pattern = "^love{0,1}$", valuetype = "regex"))
total_love_hits / ntoken(novel_lower_v, remove_punct = TRUE)
```

## 2. Make frequency plots

We are now going to obtain the ten most frequent words. Using the function dfm() we can create a matrix of counts of each word type. We can also obtain the frequency of the n most repeated words with the function textstat_frequency() 

```{r,comment=NA,warning=FALSE}
macbeth_dfm <- dfm(novel_lower_v, remove_punct = TRUE)
textstat_frequency(macbeth_dfm, n = 5)
```

As expected, the most repeated words are the most used prepositions, nouns and conjunctions in the English language.

We can also plot the frequency of the n most frequent terms. Notice that moreorless, the the 20 most repeated over a hundred times.

```{r,comment=NA,warning=FALSE}

theme_set(theme_minimal())

textstat_frequency(macbeth_dfm, n = 50) %>% 
  ggplot(aes(x = rank, y = frequency)) +
  geom_point() +
  labs(x = "Frequency rank", y = "Term frequency")

sorted_macbeth_freqs_t <- topfeatures(macbeth_dfm, n = nfeat(macbeth_dfm))
```

We can also analyze the percentage of full text and the term frequency as a percentage for the ten most frequent words.

```{r,comment=NA,warning=FALSE}
sorted_macbeth_rel_freqs_t <- sorted_macbeth_freqs_t / sum(sorted_macbeth_freqs_t) * 100
macbeth_dfm_pct <- dfm_weight(macbeth_dfm, scheme = "prop") * 100

plot(sorted_macbeth_rel_freqs_t[1:10], type = "b",
     xlab = "Top Ten Words", ylab = "Percentage of Full Text", xaxt = "n")
axis(1,1:10, labels = names(sorted_macbeth_rel_freqs_t[1:10]))

textstat_frequency(macbeth_dfm_pct, n = 10) %>% 
  ggplot(aes(x = reorder(feature, -rank), y = frequency)) +
  geom_bar(stat = "identity") + coord_flip() + 
  labs(x = "", y = "Term Frequency as a Percentage")
```

## 3. Compare word frequency data of words like “he”, “she”, “him”, “her” and show also relative frequencies

We first have to calculate the frequencies of "he", "she", "him" and "her".

```{r,comment=NA,warning=FALSE}
sorted_macbeth_freqs_t[c("he", "she", "him", "her")]
```

There exist another method to obtain the same solution, now indexing the dfm.

```{r,comment=NA,warning=FALSE}
macbeth_dfm[, c("he", "she", "him", "her")]
```

We can see clearly that the most repeated one between these words is "he", whereas the less one is "she". Therefore, without having any idea about the book we can guess that the main character is a boy and the book is written in third person or that it is written by a women which talks a lot about a men.

We also estimate its relative frequencies.

```{r,comment=NA,warning=FALSE}
sorted_macbeth_rel_freqs_t["he"]
sorted_macbeth_rel_freqs_t["she"]
sorted_macbeth_rel_freqs_t["him"]
sorted_macbeth_rel_freqs_t["her"]
```

## 4. Make a token distribution analysis

We can make token distribution analysis by doing dispersion plots. With this dispersion plot we can see the occurrences of particular terms. Let's do them by using words "Macbeth" and "Macduff".

```{r,comment=NA,warning=FALSE}
textplot_xray(kwic(novel_v, pattern = "macbeth")) + 
  ggtitle("Lexical dispersion")
```

We can see that the word "macbeth" is repeated many times through the book. Looks like this might be the main character of the book.

We are now going to compare it with another character.

```{r,comment=NA,warning=FALSE}
textplot_xray(
  kwic(novel_v, pattern = "macbeth"),
  kwic(novel_v, pattern = "macduff")) + 
  ggtitle("Lexical dispersion")
```

We can see that "macduff" has to be a secondary character in the novel. Also we can see that it appears more at the middle of the book and end rather than at the beggining. 

## 5. Identify chapter breaks

Now we are going to identify the chapter break locations. For that we are going to use the function kwic() that we have been using through all the analysis. As we know that all the chapters beging with "SCENE", we can obtain the chapter breaks straightforward.

```{r,comment=NA,warning=FALSE}
chap_positions_v <- kwic(novel_v, phrase(c("SCENE")), valuetype = "regex")$from
chap_positions_v
```

Then we can save our chapter breaks in the variable "chapters_corp".

```{r,comment=NA,warning=FALSE}
chapters_corp <- 
  corpus(novel_v) %>%
  corpus_segment(pattern = "SCENE\\s*.*\\n", valuetype = "regex")
summary(chapters_corp, 10)
docvars(chapters_corp, "pattern") <- stringi::stri_trim_right(docvars(chapters_corp, "pattern"))
docnames(chapters_corp) <- docvars(chapters_corp, "pattern")
```

## 7. Show some measures of lexical variety

With ndoc(), we obtain the number of chapters included in the book.

```{r,comment=NA,warning=FALSE}
ndoc(chapters_corp)
```

With docnames we can obtain chapter names.

```{r,comment=NA,warning=FALSE}
docnames(chapters_corp) %>% head(4)
```

Using the function ntoken() we can calculate the size of the vocabulary for each chapter. However, using ntype() the count is more precise as it counts unique words and excludes numbers and symbols.

```{r,comment=NA,warning=FALSE}
ntoken(chapters_corp) %>% head(2)
```

```{r}
ntype(chapters_corp,remove_punct = TRUE) %>% head(2)
```

As expected, the number of words using ntype() is lower than the one obtained with ntoken. We can check the difference in proportions:

```{r,comment=NA,warning=FALSE}
(ntoken(chapters_corp) / ntype(chapters_corp)) %>% head()
```

Then, we can also plot the proportion.

```{r,comment=NA,warning=FALSE}
(ntoken(chapters_corp) / ntype(chapters_corp)) %>%
  plot(type = "h", ylab = "Mean word frequency")
```

As we can see each chapter have a very different proportion than the other. therefore there must be very different in terms of length and vocabulary used.

We can also rank the chapters to see which ones are bigger and the Type-Token Ratio (TTR), which is a measure of lexical diversity calculated using textstat_lexdiv().

```{r,comment=NA,warning=FALSE}
mean_word_use_m <- (ntoken(chapters_corp) / ntype(chapters_corp))
sort(mean_word_use_m, decreasing = TRUE) %>% head()
```

And finally, we calculate the TTR.

```{r,comment=NA,warning=FALSE}
dfm(chapters_corp) %>% 
  textstat_lexdiv(measure = "TTR") %>%
  head(n = 10)
```

## 8. Calculate the Hapax Richness

The last analysis that we will do is de Hapax Richness. It is defined as the number of unique words divided by the total number of words. To calculate it we first split it by chapters.

```{r,comment=NA,warning=FALSE}
chap_dfm <- dfm(chapters_corp)
```

We calculate hapaxes per chapter.

```{r,comment=NA,warning=FALSE}
rowSums(chap_dfm == 1) %>% head()
```

And we calculate them again as a proportion.

```{r,comment=NA,warning=FALSE}
hapax_proportion <- rowSums(chap_dfm == 1) / ntoken(chap_dfm)
barplot(hapax_proportion, beside = TRUE, col = "grey", names.arg = seq_len(ndoc(chap_dfm)))
```